# -*- coding: utf-8 -*-
"""task1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18Ocnqze-N-2_P3z3qes7mqMoueo_eLv_
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""Now we will get the data and information about the dataset."""

data=pd.read_csv('titanic.csv')
data

"""**Using the info() method we will get the details about the dataset. By this we will understand that how many null values are present in the dataset.**"""

data.info()

"""**In the dataset we find null values in age and embarked column. So we will start to fill the values in the data using statistics measures.**"""

age=data['Age'].mean()
age

data['Age'].fillna(age,inplace=True)
data.info()

data['Cabin']

embark=data['Embarked'].mode()[0]
embark

data['Embarked'].fillna(embark,inplace=True)
data.info()

"""**After filliing the values we will now form the X and Y dataset for making the prediction easy. In the X dataset we will have columns which will have the parameters that help us in making the prediction.**"""

x=pd.DataFrame(data.loc[: ,
                     ['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']])
x.info()

y=pd.DataFrame(data.loc[:,['Survived']])
y.info()

y.Survived.unique()
#Using this method we will come to know the different classes in the survived column.

sns.barplot(x=x.Sex,y=y.Survived)
#It shoes the relation between survived rate in male and female.

sns.barplot(x=x.Pclass,y=y.Survived)
#It shoes the relation between survived rate in different classes.

fare=pd.cut(x['Fare'],4)
sns.barplot(x=fare,y=y.Survived)
#It shoes the relation between survived rate in different types of fare.

"""**In our data we will have categorical values and we will have some values in characters. So to train model without any problem we need to preprocess the data. For that we will use preprocessing form sklearn .**"""

from sklearn import preprocessing
labels=preprocessing.LabelEncoder().fit(x['Sex'])
x['Sex']=labels.transform(x['Sex'])
x.info()

labele=preprocessing.LabelEncoder().fit(x['Embarked'])
x['Embarked']=labele.transform(x['Embarked'])
x.info()

"""**Now we will check the correlation of the classes in X with that of the classes in Y dataset.**"""

x.corrwith(y.Survived, method='pearson')

"""**After all the preprocessing of the data, the dataset is ready for training a model. We will use a decision tree for getting the prediction. We will now train the data .**"""

from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest=train_test_split(x,y, random_state=35, test_size=0.25)
xtrain

xtest

ytrain

ytest

"""**After training the data we will now fit the data in the decision tree classifier. **"""

from sklearn.tree import DecisionTreeClassifier
dt=DecisionTreeClassifier(criterion='entropy',random_state=1)
dt.fit(xtrain,ytrain)

"""**The below method helps us in knowing the size of the decision tree.**"""

from sklearn import tree
plt.figure(figsize = ( 25 , 20) , dpi = 300.0)
_ =tree.plot_tree( dt,
                   feature_names = ['Pclass' ,'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'],
                   class_names = ['Not Survived' , 'Survived'] ,
                   filled = True)

"""Now let's find the accuracy."""

print('The depth of the tree is:',dt.get_depth())
print('No.of leaves in tree is : ',dt.get_n_leaves())

"""**The above decision tree has more number of leaves leading the model to cause overfitting. So to overcome that we need to prune the tree**"""

from sklearn.metrics import accuracy_score
pred=dt.predict(xtest)
accuracy=accuracy_score(ytest,pred)
print('The accuracy of the tree is:',accuracy*100)

"""**To improve the accuracy we need to prune the tree.**"""

from sklearn.model_selection import GridSearchCV
params={'criterion':['entropy','gini'],'max_depth':range(1,13)}
grid_search = GridSearchCV(estimator = dt,
                            param_grid = params,
                           scoring = 'accuracy',
                           cv = 10,
                           n_jobs = -1)

# Apply fit() method to construct different decision trees with given parameters

grid_search = grid_search.fit(xtrain, ytrain)
grid_search.best_estimator_

"""**When we prune the tree we will get the parameters for designing another model for the tree.**"""

dtc=DecisionTreeClassifier(criterion='entropy',max_depth=3,random_state=1)
dtc.fit(xtrain,ytrain)

plt.figure(figsize = ( 25 , 20) , dpi = 300.0)
_ =tree.plot_tree( dtc,
                   feature_names = ['Pclass' ,'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'],
                   class_names = ['Not Survived' , 'Survived'] ,
                   filled = True)

from sklearn.metrics import confusion_matrix
predy=dtc.predict(xtest)
modified_acc=accuracy_score(ytest,predy)
print('The accuracy of the model is:',modified_acc*100)
cm=confusion_matrix(ytest,predy)
sns.heatmap(cm, annot = True , fmt = 'd')

"""**When we prune and train the tree again we can easily improve the accuracy. Then we will give the new data for checking wether the person survives or not.**"""

new_data = {'Pclass': [1], 'Sex': [0], 'Age': [25], 'SibSp': [2], 'Parch':[2],
            'Fare': [300.0], 'Embarked': [2]}

new_df = pd.DataFrame(new_data)

new_pred = dtc.predict(new_df)

if new_pred == 0:
    print('Prediction: The passenger will not survive')
else:
    print('Prediction: The passenger will survive')